{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mustererkennung in Funkmessdaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1: Laden der Datenbank in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir öffnen die Datenbank und lassen uns die Keys der einzelnen Tabellen ausgeben. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hdf = pd.HDFStore('../../data/raw/TestMessungen_NEU.hdf')\n",
    "print(hdf.keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2: Inspektion eines einzelnen Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir laden den Frame x1_t1_trx_1_4 und betrachten seine Dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_x1_t1_trx_1_4 = hdf.get('/x1/t1/trx_1_4')\n",
    "print(\"Rows:\", df_x1_t1_trx_1_4.shape[0])\n",
    "print(\"Columns:\", df_x1_t1_trx_1_4.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes Untersuchen wir exemplarisch für zwei Empfänger-Sender-Gruppen die Attributzusammensetzung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first inspection of columns from df_x1_t1_trx_1_4\n",
    "df_x1_t1_trx_1_4.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Analyse der Frames definieren wir einige Hilfsfunktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Little function to retrieve sender-receiver tuples from df columns\n",
    "def extract_snd_rcv(df):\n",
    "    regex = r\"trx_[1-4]_[1-4]\"\n",
    "    # creates a set containing the different pairs\n",
    "    snd_rcv = {x[4:7] for x in df.columns if re.search(regex, x)}\n",
    "    return [(x[0],x[-1]) for x in snd_rcv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sums the number of columns for each sender-receiver tuple\n",
    "def get_column_counts(snd_rcv, df):\n",
    "    col_counts = {}\n",
    "    for snd,rcv in snd_rcv:\n",
    "        col_counts['Columns for pair {} {}:'.format(snd, rcv)] = len([i for i, word in enumerate(list(df.columns)) if word.startswith('trx_{}_{}'.format(snd, rcv))])\n",
    "    return col_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Analyze the column composition of a given measurement.\n",
    "def analyse_columns(df):\n",
    "    df_snd_rcv = extract_snd_rcv(df)\n",
    "    cc = get_column_counts(df_snd_rcv, df)\n",
    "\n",
    "    for x in cc:\n",
    "        print(x, cc[x])\n",
    "    print(\"Sum of pair related columns: %i\" % sum(cc.values()))\n",
    "    print()\n",
    "    print(\"Other columns are:\")\n",
    "    for att in [col for col in df.columns if 'ifft' not in col and 'ts' not in col]:\n",
    "        print(att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Analyze the values of the target column.\n",
    "def analyze_target(df):\n",
    "    print(df['target'].unique())\n",
    "    print(\"# Unique values in target: %i\" % len(df['target'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bestimme nun die Spaltezusammensetzung von df_x1_t1_trx_1_4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analyse_columns(df_x1_t1_trx_1_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Betrachte den Inhalt der \"target\"-Spalte von df_x1_t1_trx_1_4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analyze_target(df_x1_t1_trx_1_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes laden wir den Frame x3_t2_trx_3_1 und betrachten seine Dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_x3_t2_trx_3_1 = hdf.get('/x3/t2/trx_3_1')\n",
    "print(\"Rows:\", df_x3_t2_trx_3_1.shape[0])\n",
    "print(\"Columns:\", df_x3_t2_trx_3_1.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Gefolgt von einer Analyse seiner Spaltenzusammensetzung und seiner \"target\"-Werte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analyse_columns(df_x3_t2_trx_3_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analyze_target(df_x3_t2_trx_3_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frage: Was stellen Sie bzgl. der „Empfänger-Nummer_Sender-Nummer“-Kombinationen fest? Sind diese gleich? Welche Ausprägungen finden Sie in der Spalte „target“? \n",
    "\n",
    "Antwort: Wir sehen, wenn jeweils ein Paar sendet, hören die anderen beiden Sender zu und messen ihre Verbindung zu den gerade sendenden Knoten (d.h. 6 Paare in jedem Dataframe). Sendet z.B. das Paar 3 1, so misst Knoten 1 die Verbindung 1-3, Knoten 3 die Verbindung 3-1 und Knoten 2 und 4 Verbindung 2-1 und 2-3 bzw. 4-1 und 4-3. Die 10 verschiedenen Ausprägungen der Spalte \"target\" sind oben zu sehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 3: Visualisierung der Messreihe des Datensatz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir visualisieren die Rohdaten mit verschiedenen Heatmaps, um so die Integrität der Daten optisch zu validieren und Ideen für mögliche Features zu entwickeln. Hier stellen wir exemplarisch die Daten von Frame df_x1_t1_trx_1_4 dar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vals = df_x1_t1_trx_1_4.loc[:,'trx_2_4_ifft_0':'trx_2_4_ifft_1999'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# one big heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "plt.title('trx_2_4_ifft')\n",
    "plt.xlabel(\"ifft of frequency\")\n",
    "plt.ylabel(\"measurement\")\n",
    "ax = sns.heatmap(vals, xticklabels=200, yticklabels=20, vmin=0, vmax=1, cmap='nipy_spectral_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir betrachten wie verschiedene Farbschemata unterschiedliche Merkmale unserer Rohdaten hervorheben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compare different heatmaps\n",
    "plt.figure(1, figsize=(12,10))\n",
    "\n",
    "# nipy_spectral_r scheme\n",
    "plt.subplot(221)\n",
    "plt.title('trx_2_4_ifft')\n",
    "plt.xlabel(\"ifft of frequency\")\n",
    "plt.ylabel(\"measurement\")\n",
    "ax = sns.heatmap(vals, xticklabels=200, yticklabels=20, vmin=0, vmax=1, cmap='nipy_spectral_r')\n",
    "\n",
    "# terrain scheme\n",
    "plt.subplot(222)\n",
    "plt.title('trx_2_4_ifft')\n",
    "plt.xlabel(\"ifft of frequency\")\n",
    "plt.ylabel(\"measurement\")\n",
    "ax = sns.heatmap(vals, xticklabels=200, yticklabels=20, vmin=0, vmax=1, cmap='terrain')\n",
    "\n",
    "# Vega10 scheme\n",
    "plt.subplot(223)\n",
    "plt.title('trx_2_4_ifft')\n",
    "plt.xlabel(\"ifft of frequency\")\n",
    "plt.ylabel(\"measurement\")\n",
    "ax = sns.heatmap(vals, xticklabels=200, yticklabels=20, vmin=0, vmax=1, cmap='Vega10')\n",
    "\n",
    "# Wistia scheme\n",
    "plt.subplot(224)\n",
    "plt.title('trx_2_4_ifft')\n",
    "plt.xlabel(\"ifft of frequency\")\n",
    "plt.ylabel(\"measurement\")\n",
    "ax = sns.heatmap(vals, xticklabels=200, yticklabels=20, vmin=0, vmax=1, cmap='Wistia')\n",
    "\n",
    "# Adjust the subplot layout, because the logit one may take more space\n",
    "# than usual, due to y-tick labels like \"1 - 10^{-3}\"\n",
    "plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n",
    "                    wspace=0.2)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 3: Groundtruth-Label anpassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Iterating over hdf data and creating interim data presentation stored in data/interim/testmessungen_interim.hdf\n",
    "# Interim data representation contains aditional binary class (binary_target - encoding 0=empty and 1=not empty)\n",
    "# and multi class target (multi_target - encoding 0-9 for each possible class)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "interim_path = '../../data/interim/01_testmessungen.hdf'\n",
    "\n",
    "def binary_mapper(df):\n",
    "    \n",
    "    def map_binary(target):\n",
    "        if target.startswith('Empty'):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    df['binary_target'] = pd.Series(map(map_binary, df['target']))\n",
    "    \n",
    "    \n",
    "def multiclass_mapper(df):\n",
    "    le.fit(df['target'])\n",
    "    df['multi_target'] = le.transform(df['target'])\n",
    "    \n",
    "for key in hdf.keys():\n",
    "    df = hdf.get(key)\n",
    "    binary_mapper(df)\n",
    "    multiclass_mapper(df)\n",
    "    df.to_hdf(interim_path, key)\n",
    "\n",
    "hdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Überprüfe neu beschrifteten Dataframe „/x1/t1/trx_3_1“ verwenden. Wir erwarten als Ergebnisse für 5 zu Beginn des Experiments „Empty“ (bzw. 0) und für 120 mitten im Experiment „Not Empty“ (bzw. 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hdf = pd.HDFStore('../../data/interim/01_testmessungen.hdf')\n",
    "df_x1_t1_trx_3_1 = hdf.get('/x1/t1/trx_3_1')\n",
    "print(\"binary_target for measurement 5:\", df_x1_t1_trx_3_1['binary_target'][5])\n",
    "print(\"binary_target for measurement 120:\", df_x1_t1_trx_3_1['binary_target'][120])\n",
    "hdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 4: Einfacher Erkenner mit Hold-Out-Validierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir folgen den Schritten in Aufgabe 4 und testen einen einfachen Erkenner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from evaluation import *\n",
    "from filters import *\n",
    "from utility import *\n",
    "from features import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Öffnen von Hdf mittels pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# raw data to achieve target values\n",
    "hdf = pd.HDFStore('../../data/raw/TestMessungen_NEU.hdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beispiel Erkenner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datensätze vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate datasets\n",
    "tst = ['1','2','3']\n",
    "tst_ds = []\n",
    "\n",
    "for t in tst:\n",
    "\n",
    "    df_tst = hdf.get('/x1/t'+t+'/trx_3_1')\n",
    "    lst = df_tst.columns[df_tst.columns.str.contains('_ifft_')]\n",
    "    \n",
    "    #df_tst_cl,_ = distortion_filter(df_tst_cl)\n",
    "    \n",
    "    groups = get_trx_groups(df_tst)\n",
    "    df_std = rf_grouped(df_tst, groups=groups, fn=rf_std_single, label='target')\n",
    "    df_mean = rf_grouped(df_tst, groups=groups, fn=rf_mean_single)\n",
    "    df_p2p = rf_grouped(df_tst, groups=groups, fn=rf_ptp_single) # added p2p feature\n",
    "    \n",
    "    df_all = pd.concat( [df_std, df_mean, df_p2p], axis=1 ) # added p2p feature\n",
    "    \n",
    "    df_all = cf_std_window(df_all, window=4, label='target')\n",
    "    \n",
    "    df_tst_sum = generate_class_label_presence(df_all, state_variable='target')\n",
    "    \n",
    "    # remove index column\n",
    "    df_tst_sum = df_tst_sum[df_tst_sum.columns.values[~df_tst_sum.columns.str.contains('index')].tolist()]\n",
    "    print('Columns in Dataset:',t)\n",
    "    print(df_tst_sum.columns)\n",
    "    \n",
    "    tst_ds.append(df_tst_sum.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# holdout validation\n",
    "print(hold_out_val(tst_ds, target='target', include_self=False, cl='rf', verbose=False, random_state=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schließen von HDF Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Aufgabe 5: Eigener Erkenner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Konstruktion eines eigenen Erkenners führen wir die entsprechenden Preprocessing und Mapping Schritte ausgehend von den Roddaten erneut durch und passen diese unseren Bedürfnissen an.# Load hdfs data\n",
    "hdfs = pd.HDFStore(\"../../data/raw/henrik/TestMessungen_NEU.hdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "hdf = pd.HDFStore(\"../../data/raw/TestMessungen_NEU.hdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check available keys in hdf store\n",
    "print(hdf.keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorverarbeitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zuerst passen wir die Groundtruth-Label an, entfernen Zeitstempel sowie Zeilenindices und speichern die resultierenden Frames ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mapping groundtruth to 0-empty and 1-not empty and prepare for further preprocessing by\n",
    "# removing additional timestamp columns and index column\n",
    "# Storing cleaned dataframes (no index, removed _ts columns, mapped multi classes to 0-empty, 1-not empty)\n",
    "# to new hdfstore to `data/interim/02_testmessungen.hdf`\n",
    "\n",
    "hdf_path = \"../../data/interim/02_tesmessungen.hdf\"\n",
    "\n",
    "dfs = []\n",
    "for key in hdf.keys():\n",
    "    df = hdf.get(key)\n",
    "    #df['target'] = df['target'].map(lambda x: 0 if x.startswith(\"Empty\") else 1)  \n",
    "    # drop all time stamp columns who endswith _ts\n",
    "    cols = [c for c in df.columns if not c.lower().endswith(\"ts\")]\n",
    "    df = df[cols]\n",
    "    df = df.drop('Timestamp', axis=1)\n",
    "    df = df.drop('index', axis=1)\n",
    "    df.to_hdf(hdf_path, key)\n",
    "hdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir sehen, dass nur noch die 6 x 2000 Messungen für die jeweiligen Paare sowie die 'target'-Werte in den resultierenden Frames enthalten sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf = pd.HDFStore(hdf_path)\n",
    "df = hdf.get(\"/x1/t1/trx_1_2\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step-1 repeating the previous taks 4 to get a comparable base result with the now dropped _ts and index column to improve from\n",
    "# generate datasets\n",
    "from evaluation import *\n",
    "from filters import *\n",
    "from utility import *\n",
    "from features import *\n",
    "\n",
    "\n",
    "tst = ['1','2','3']\n",
    "tst_ds = []\n",
    "\n",
    "for t in tst:\n",
    "\n",
    "    df_tst = hdf.get('/x1/t'+t+'/trx_3_1')\n",
    "    lst = df_tst.columns[df_tst.columns.str.contains('_ifft_')]\n",
    "    \n",
    "    #df_tst_cl,_ = distortion_filter(df_tst_cl)\n",
    "    \n",
    "    df_tst,_ = distortion_filter(df_tst)\n",
    "    \n",
    "    groups = get_trx_groups(df_tst)\n",
    "    df_std = rf_grouped(df_tst, groups=groups, fn=rf_std_single, label='target')\n",
    "    df_mean = rf_grouped(df_tst, groups=groups, fn=rf_mean_single)\n",
    "    \n",
    "    df_p2p = rf_grouped(df_tst, groups=groups, fn=rf_ptp_single) # added p2p feature\n",
    "    \n",
    "    df_kurt = rf_grouped(df_tst, groups=groups, fn=rf_kurtosis_single)\n",
    "    \n",
    "    df_all = pd.concat( [df_std, df_mean, df_p2p, df_kurt], axis=1 ) # added p2p feature\n",
    "     \n",
    "    df_all = cf_std_window(df_all, window=4, label='target')\n",
    "    \n",
    "    df_all = cf_diff(df_all, label='target')\n",
    "    \n",
    "    df_tst_sum = generate_class_label_presence(df_all, state_variable='target')\n",
    "    \n",
    "    # remove index column\n",
    "    # df_tst_sum = df_tst_sum[df_tst_sum.columns.values[~df_tst_sum.columns.str.contains('index')].tolist()]\n",
    "    # print('Columns in Dataset:',t)\n",
    "    # print(df_tst_sum.columns)\n",
    "    \n",
    "    tst_ds.append(df_tst_sum.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluating different supervised learning methods provided in eval.py\n",
    "# added a NN evaluator but there are some problems regarding usage and hidden layers\n",
    "# For the moment only kurtosis and cf_diff are added to the dataset as well as the distortion filter\n",
    "# Feature selection is needed right now!\n",
    "for elem in ['rf', 'dt', 'nb' ,'nn','knn']:\n",
    "    print(elem, \":\", hold_out_val(tst_ds, target='target', include_self=False, cl=elem, verbose=False, random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_models(ds):\n",
    "    res = {}\n",
    "    for elem in ['rf', 'dt', 'nb' ,'nn','knn']: \n",
    "        res[elem] = hold_out_val(ds, target='target', include_self=False, cl=elem, verbose=False, random_state=1)\n",
    "    return res\n",
    "\n",
    "def prepare_features(c, p):\n",
    "    tst = ['1','2','3']\n",
    "    tst_ds = []\n",
    "\n",
    "    for t in tst:\n",
    "\n",
    "        df_tst = hdf.get('/x'+c+'/t'+t+'/trx_'+p)\n",
    "        lst = df_tst.columns[df_tst.columns.str.contains('_ifft_')]\n",
    "\n",
    "        #df_tst_cl,_ = distortion_filter(df_tst_cl)\n",
    "\n",
    "        df_tst,_ = distortion_filter(df_tst)\n",
    "\n",
    "        groups = get_trx_groups(df_tst)\n",
    "        df_std = rf_grouped(df_tst, groups=groups, fn=rf_std_single, label='target')\n",
    "        df_mean = rf_grouped(df_tst, groups=groups, fn=rf_mean_single)\n",
    "\n",
    "        df_p2p = rf_grouped(df_tst, groups=groups, fn=rf_ptp_single) # added p2p feature\n",
    "\n",
    "        df_kurt = rf_grouped(df_tst, groups=groups, fn=rf_kurtosis_single)\n",
    "\n",
    "        df_all = pd.concat( [df_std, df_mean, df_p2p, df_kurt], axis=1 ) # added p2p feature\n",
    "\n",
    "        df_all = cf_std_window(df_all, window=4, label='target')\n",
    "\n",
    "        df_all = cf_diff(df_all, label='target')\n",
    "\n",
    "        df_tst_sum = generate_class_label_presence(df_all, state_variable='target')\n",
    "\n",
    "        # remove index column\n",
    "        # df_tst_sum = df_tst_sum[df_tst_sum.columns.values[~df_tst_sum.columns.str.contains('index')].tolist()]\n",
    "        # print('Columns in Dataset:',t)\n",
    "        # print(df_tst_sum.columns)\n",
    "\n",
    "        tst_ds.append(df_tst_sum.copy())\n",
    "        \n",
    "    return tst_ds\n",
    "    \n",
    "def evaluate_performance(c, p):\n",
    "    # include a prepare data function?\n",
    "    ds = prepare_features(c, p)\n",
    "    return evaluate_models(ds)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = ['1','2','3','4']\n",
    "pairing = ['1_2','1_4','2_3','3_1','3_4','4_2']\n",
    "tst_ds = []\n",
    "\n",
    "for c in config:\n",
    "    print(\"Testing for configuration\", c)\n",
    "    for p in pairing:\n",
    "        print(\"Analyse performance for pairing\", p)\n",
    "        res = evaluate_performance(c, p)\n",
    "        # TODO draw graph\n",
    "        for model in res:\n",
    "            print(model, res[model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 6: Online Erkenner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Serialisierung des Models für den Online Predictor\n",
    "Das zuvor gewählte Model wird serialisiert und in 'models/solution_ueb02' gespeichert damit es beim starten der REST-API geladen werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(res['dt'], '../../models/solution_ueb02/model.plk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starten des online servers\n",
    "#### Hierzu müssen die Abhängigkeiten Flask, flask_restful, flask_cors installiert sein\n",
    "The following command starts a flask_restful server on localhost port:5444 which answers json post requests. The server is implemented in the file online.py within the ipynb folder and makes use of the final chosen model.\n",
    "Requests can be made as post request to http://localhost:5444/predict with a json file of the following format:\n",
    "{ \"row\": \"features\" }\n",
    "be careful that the sent file is valid json. The answer contains the predicted class.\n",
    "{ \"p_class\": \"predicted class\" }\n",
    "For now the online predictor only predicts the class of single lines sent to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Navigate to notebooks/solution_ueb02 and start the server\n",
    "# with 'python -m online'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nun werden zeilenweise Anfragen an die REST-API simuliert, jeder valider json request wird mit einer\n",
    "# json prediction response beantwortet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
