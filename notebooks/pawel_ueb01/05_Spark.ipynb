{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Excercise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\n",
    "dataset = pd.read_csv(url, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create Spark Dataframe from Pandas Dataframe\n",
    "cancer_df = sqlContext.createDataFrame(dataset, schema=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After parsing, number of training lines: 569\n",
      "root\n",
      " |-- 0: long (nullable = true)\n",
      " |-- 1: string (nullable = true)\n",
      " |-- 2: double (nullable = true)\n",
      " |-- 3: double (nullable = true)\n",
      " |-- 4: double (nullable = true)\n",
      " |-- 5: double (nullable = true)\n",
      " |-- 6: double (nullable = true)\n",
      " |-- 7: double (nullable = true)\n",
      " |-- 8: double (nullable = true)\n",
      " |-- 9: double (nullable = true)\n",
      " |-- 10: double (nullable = true)\n",
      " |-- 11: double (nullable = true)\n",
      " |-- 12: double (nullable = true)\n",
      " |-- 13: double (nullable = true)\n",
      " |-- 14: double (nullable = true)\n",
      " |-- 15: double (nullable = true)\n",
      " |-- 16: double (nullable = true)\n",
      " |-- 17: double (nullable = true)\n",
      " |-- 18: double (nullable = true)\n",
      " |-- 19: double (nullable = true)\n",
      " |-- 20: double (nullable = true)\n",
      " |-- 21: double (nullable = true)\n",
      " |-- 22: double (nullable = true)\n",
      " |-- 23: double (nullable = true)\n",
      " |-- 24: double (nullable = true)\n",
      " |-- 25: double (nullable = true)\n",
      " |-- 26: double (nullable = true)\n",
      " |-- 27: double (nullable = true)\n",
      " |-- 28: double (nullable = true)\n",
      " |-- 29: double (nullable = true)\n",
      " |-- 30: double (nullable = true)\n",
      " |-- 31: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'After parsing, number of training lines: {}'.format(cancer_df.count())\n",
    "cancer_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Pipeline stages\n",
    "stages = []\n",
    "\n",
    "\n",
    "# Convert label into label indices using StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol=\"1\", outputCol = \"label\")\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform all numerical features into a vector using VectorAssembler\n",
    "\n",
    "# numeric cols 2:31\n",
    "numeric_cols = [\"{}\".format(x) for x in range(2,32)]\n",
    "assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  1.0|[17.99,10.38,122....|\n",
      "|  1.0|[20.57,17.77,132....|\n",
      "|  1.0|[19.69,21.25,130....|\n",
      "|  1.0|[11.42,20.38,77.5...|\n",
      "|  1.0|[20.29,14.34,135....|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "pipeline_model = pipeline.fit(cancer_df)\n",
    "cancer_transformed_df = pipeline_model.transform(cancer_df)\n",
    "\n",
    "\n",
    "\n",
    "# Keep relevant columns\n",
    "selected_cols = [\"label\", \"features\"]\n",
    "cancer_final_df = cancer_transformed_df.select(selected_cols)\n",
    "cancer_final_df.printSchema()\n",
    "cancer_final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- scaledFeatures: vector (nullable = true)\n",
      "\n",
      "+-----+--------------------+--------------------+\n",
      "|label|            features|      scaledFeatures|\n",
      "+-----+--------------------+--------------------+\n",
      "|  1.0|[17.99,10.38,122....|[5.10492359418784...|\n",
      "|  1.0|[20.57,17.77,132....|[5.83703603849048...|\n",
      "|  1.0|[19.69,21.25,130....|[5.58732326679036...|\n",
      "|  1.0|[11.42,20.38,77.5...|[3.24059074183575...|\n",
      "|  1.0|[20.29,14.34,135....|[5.75758197476772...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "\n",
    "scalerModel = scaler.fit(cancer_final_df)\n",
    "\n",
    "#normalize each feature to have unit stdev\n",
    "scaled_cancer_final_df = scalerModel.transform(cancer_final_df)\n",
    "scaled_cancer_final_df.printSchema()\n",
    "scaled_cancer_final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled_cancer_final_df = scaled_cancer_final_df.selectExpr(\"label\", \"scaledFeatures as features\")\n",
    "scaled_cancer_final_df.printSchema()\n",
    "scaled_cancer_final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421\n",
      "148\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = cancer_final_df.randomSplit([0.7, 0.3], seed = 1)\n",
    "\n",
    "print trainingData.count()\n",
    "print testData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  1.0|[20.57,17.77,132....|[-2.0316470979350...|[0.11592001643133...|       1.0|\n",
      "|  1.0|[20.29,14.34,135....|[-2.5181959189331...|[0.07459238238578...|       1.0|\n",
      "|  1.0|[18.25,19.98,119....|[-2.3243425285033...|[0.08912688901575...|       1.0|\n",
      "|  1.0|[16.02,23.24,102....|[0.08587805337901...|[0.52145632819203...|       0.0|\n",
      "|  1.0|[15.78,17.89,103....|[-1.9185235212382...|[0.12802630336040...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a LogisticRegression model.\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "# Train model\n",
    "lr_model = lr.fit(trainingData)\n",
    "# make predictions\n",
    "predictions = lr_model.transform(testData)\n",
    "# select example rows to display\n",
    "predictions.show(5)\n",
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderROC : 0.999057848125\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "#Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print(str(evaluator.getMetricName()) + \" : \" + str(evaluator.evaluate(predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "featuresCol: features column name (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name (default: label, current: label)\n",
      "maxIter: max number of iterations (>= 0) (default: 100, current: 10)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "regParam: regularization parameter (>= 0) (default: 0.1)\n",
      "threshold: threshold in binary classification prediction, in range [0, 1]. (default: 0.5)\n",
      "tol: the convergence tolerance for iterative algorithms (default: 1e-06)\n"
     ]
    }
   ],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "                .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "                .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "                .addGrid(lr.maxIter, [1, 10, 100])\n",
    "                .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cv\n",
    "cvModel = cv.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions2 = cvModel.transform(testData)\n",
    "predictions2.show(5)\n",
    "predictions2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(str(evaluator.getMetricName()) + \" : \" + str(evaluator.evaluate(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
