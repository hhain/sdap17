{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Excercise 5 Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\n",
    "dataset = pd.read_csv(url, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create Spark Dataframe from Pandas Dataframe\n",
    "cancer_df = sqlContext.createDataFrame(dataset, schema=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After parsing, number of training lines: 569\n",
      "root\n",
      " |-- 0: long (nullable = true)\n",
      " |-- 1: string (nullable = true)\n",
      " |-- 2: double (nullable = true)\n",
      " |-- 3: double (nullable = true)\n",
      " |-- 4: double (nullable = true)\n",
      " |-- 5: double (nullable = true)\n",
      " |-- 6: double (nullable = true)\n",
      " |-- 7: double (nullable = true)\n",
      " |-- 8: double (nullable = true)\n",
      " |-- 9: double (nullable = true)\n",
      " |-- 10: double (nullable = true)\n",
      " |-- 11: double (nullable = true)\n",
      " |-- 12: double (nullable = true)\n",
      " |-- 13: double (nullable = true)\n",
      " |-- 14: double (nullable = true)\n",
      " |-- 15: double (nullable = true)\n",
      " |-- 16: double (nullable = true)\n",
      " |-- 17: double (nullable = true)\n",
      " |-- 18: double (nullable = true)\n",
      " |-- 19: double (nullable = true)\n",
      " |-- 20: double (nullable = true)\n",
      " |-- 21: double (nullable = true)\n",
      " |-- 22: double (nullable = true)\n",
      " |-- 23: double (nullable = true)\n",
      " |-- 24: double (nullable = true)\n",
      " |-- 25: double (nullable = true)\n",
      " |-- 26: double (nullable = true)\n",
      " |-- 27: double (nullable = true)\n",
      " |-- 28: double (nullable = true)\n",
      " |-- 29: double (nullable = true)\n",
      " |-- 30: double (nullable = true)\n",
      " |-- 31: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'After parsing, number of training lines: {}'.format(cancer_df.count())\n",
    "cancer_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Pipeline stages\n",
    "stages = []\n",
    "\n",
    "\n",
    "# Convert label into label indices using StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol=\"1\", outputCol = \"label\")\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform all numerical features into a vector using VectorAssembler\n",
    "\n",
    "# numeric cols 2:31\n",
    "numeric_cols = [\"{}\".format(x) for x in range(2,32)]\n",
    "assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  1.0|[17.99,10.38,122....|\n",
      "|  1.0|[20.57,17.77,132....|\n",
      "|  1.0|[19.69,21.25,130....|\n",
      "|  1.0|[11.42,20.38,77.5...|\n",
      "|  1.0|[20.29,14.34,135....|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "pipeline_model = pipeline.fit(cancer_df)\n",
    "cancer_transformed_df = pipeline_model.transform(cancer_df)\n",
    "\n",
    "\n",
    "\n",
    "# Keep relevant columns\n",
    "selected_cols = [\"label\", \"features\"]\n",
    "cancer_final_df = cancer_transformed_df.select(selected_cols)\n",
    "cancer_final_df.printSchema()\n",
    "cancer_final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- scaledFeatures: vector (nullable = true)\n",
      "\n",
      "+-----+--------------------+--------------------+\n",
      "|label|            features|      scaledFeatures|\n",
      "+-----+--------------------+--------------------+\n",
      "|  1.0|[17.99,10.38,122....|[5.10492359418784...|\n",
      "|  1.0|[20.57,17.77,132....|[5.83703603849048...|\n",
      "|  1.0|[19.69,21.25,130....|[5.58732326679036...|\n",
      "|  1.0|[11.42,20.38,77.5...|[3.24059074183575...|\n",
      "|  1.0|[20.29,14.34,135....|[5.75758197476772...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "\n",
    "scalerModel = scaler.fit(cancer_final_df)\n",
    "\n",
    "#normalize each feature to have unit stdev\n",
    "scaled_cancer_final_df = scalerModel.transform(cancer_final_df)\n",
    "scaled_cancer_final_df.printSchema()\n",
    "scaled_cancer_final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  1.0|[5.10492359418784...|\n",
      "|  1.0|[5.83703603849048...|\n",
      "|  1.0|[5.58732326679036...|\n",
      "|  1.0|[3.24059074183575...|\n",
      "|  1.0|[5.75758197476772...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Substitute fetures with newly created scaledFeatures \n",
    "# (Spark Dataframe columns cannot be modified but a copy can be created and in next step a name can be changed)\n",
    "scaled_cancer_final_df = scaled_cancer_final_df.selectExpr(\"label\", \"scaledFeatures as features\")\n",
    "scaled_cancer_final_df.printSchema()\n",
    "scaled_cancer_final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420\n",
      "149\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = cancer_final_df.randomSplit([0.7, 0.3], seed = 1)\n",
    "\n",
    "print trainingData.count()\n",
    "print testData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  1.0|[20.57,17.77,132....|[-1.9032279000039...|[0.12974357579949...|       1.0|\n",
      "|  1.0|[20.29,14.34,135....|[-2.6720690064567...|[0.06464175711414...|       1.0|\n",
      "|  1.0|[18.25,19.98,119....|[-2.1792208875297...|[0.10163204110436...|       1.0|\n",
      "|  1.0|[16.02,23.24,102....|[0.29558400444961...|[0.57336263675245...|       0.0|\n",
      "|  1.0|[15.78,17.89,103....|[-1.9584906497444...|[0.12363048673566...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a LogisticRegression model.\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "# Train model\n",
    "lr_model = lr.fit(trainingData)\n",
    "# make predictions\n",
    "predictions = lr_model.transform(testData)\n",
    "# select example rows to display\n",
    "predictions.show(5)\n",
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderROC : 0.987947269303\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "#Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print(str(evaluator.getMetricName()) + \" : \" + str(evaluator.evaluate(predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "featuresCol: features column name (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name (default: label, current: label)\n",
      "maxIter: max number of iterations (>= 0) (default: 100, current: 10)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "regParam: regularization parameter (>= 0) (default: 0.1)\n",
      "threshold: threshold in binary classification prediction, in range [0, 1]. (default: 0.5)\n",
      "tol: the convergence tolerance for iterative algorithms (default: 1e-06)\n"
     ]
    }
   ],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets test the Cross Validator with a parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "                .addGrid(lr.regParam, [0.01, 0.1, 0.5, 2.0])\n",
    "                .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "                .addGrid(lr.maxIter, [1, 10, 100])\n",
    "                .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cv\n",
    "cvModel = cv.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  1.0|[20.57,17.77,132....|[-3.7099755446092...|[0.02389325976166...|       1.0|\n",
      "|  1.0|[20.29,14.34,135....|[-5.2965166269030...|[0.00498404666116...|       1.0|\n",
      "|  1.0|[18.25,19.98,119....|[-4.8198505720329...|[0.00800342109037...|       1.0|\n",
      "|  1.0|[16.02,23.24,102....|[-0.0350601111901...|[0.49123586993146...|       1.0|\n",
      "|  1.0|[15.78,17.89,103....|[-4.3766108829897...|[0.01241188939894...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions2 = cvModel.transform(testData)\n",
    "predictions2.show(5)\n",
    "predictions2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderROC : 0.987947269303\n"
     ]
    }
   ],
   "source": [
    "print(str(evaluator.getMetricName()) + \" : \" + str(evaluator.evaluate(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that of of the parameter grid no parameter combination was better than the default one. Unfortunately because of very long running time of CrossValidator and issues with the PySpark Kernel no finer parameter grid has been tested. For those parameter that were the following best combination can be extracted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Param <regParam>: 0.01\n",
      "Best Param <elasticNetParam>: 0.0\n",
      "Best Param <maxIter>: 10\n"
     ]
    }
   ],
   "source": [
    "bestModel = cvModel.bestModel\n",
    "print(\"Best Param <regParam>: {}\".format(bestModel._java_obj.getRegParam()))\n",
    "print(\"Best Param <elasticNetParam>: {}\".format(bestModel._java_obj.getElasticNetParam()))\n",
    "print(\"Best Param <maxIter>: {}\".format(bestModel._java_obj.getMaxIter()))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark (Spark 1.4.1, Anaconda Python 2.7, #Exec: 17, Mem/Exec: 10GB)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
